# Server Configuration
PORT=3001

# LLM Provider Configuration
# Set to 'false' to disable Groq in auto mode (default: true)
USE_GROQ=true

# === LLM Provider API Keys ===

# Groq API Key (Llama 3.3 70B) ‚ö° RECOMMENDED
# Get your API key from: https://console.groq.com/
# FREE tier: 6,000 tokens/minute | 80-90% cheaper than Claude | 7x faster
# Cost: $0.59 input, $0.79 output per 1M tokens
GROQ_API_KEY=your_groq_api_key_here

# Google Gemini API Key (Gemini 2.0 Flash) üîÆ FREE OPTION
# Get your API key from: https://aistudio.google.com/app/apikey
# FREE tier: 250K tokens/min | 1M context window
# Cost: FREE up to 128K tokens, then $0.075 input, $0.30 output per 1M tokens
GEMINI_API_KEY=your_gemini_api_key_here

# Anthropic Claude API Key (Claude 3.5 Sonnet) üß† PREMIUM
# Get your API key from: https://console.anthropic.com/
# Highest quality analysis, premium pricing
# Cost: $3.00 input, $15.00 output per 1M tokens
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Ollama Configuration (Local LLM) üè† PRIVATE & FREE
# Ollama server URL (default: http://localhost:11434)
# Install: https://ollama.com/download
# 100% free, runs locally, no API needed, complete privacy
# Recommended models: llama3.3:70b, llama3.1:8b, mistral:7b
OLLAMA_URL=http://localhost:11434

# === Transcription Service ===

# AssemblyAI API Key (Transcript Extraction)
# Get your API key from: https://www.assemblyai.com/
ASSEMBLYAI_API_KEY=your_assemblyai_api_key_here

# === Provider Selection Guide ===
# Auto: Smart fallback (Groq ‚Üí Gemini ‚Üí Claude)
# Groq: Best value - fast and cheap
# Gemini: Free tier - great for testing
# Ollama: Local - private and free
# Claude: Premium - highest quality
